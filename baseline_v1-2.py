# -*- coding: utf-8 -*-
"""Baseline.v1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fXJ3zAM1kfBmob6aR1fN2I8u-sblNG0U

**Import packages**
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import keras
import numpy as np
np.random.seed(310) # NumPy
import random
random.seed(420) # Python
from tensorflow import set_random_seed
set_random_seed(440) # Tensorflow

from keras import applications
from keras.datasets import mnist
import matplotlib.pyplot as plt
import matplotlib as mpl
from numpy import asarray

from keras import optimizers
from keras import utils as keras_utils
from keras.layers import (Conv2D, BatchNormalization, MaxPool2D, Dense, Dropout, Flatten)

from keras.models import (Model, Sequential)
import sklearn.metrics as sk_met

from PIL import Image
import os, os.path
import time

from AdaptiveBinning import AdaptiveBinning

"""**Normalize images**"""

def normalize_images(images, H, W):
    images = np.reshape(images, (-1, H * W))/255.0
    return np.reshape(images, (-1, H, W))

"""**Load MNIST with no validation data**"""

def load_mnist_no_val(train_size, test_size, padding):
  if (train_size + test_size)>70000: #Raise an exception if too much data is requested
    raise Exception('Not enough data')

  (x_train, y_train), (x_test, y_test) = mnist.load_data()

  #Create array with all data for x & y
  x = np.concatenate((x_train, x_test),axis=0)
  y = np.concatenate((y_train, y_test),axis=0)

  W = x.shape[1] #Width of image in pixels
  H = x.shape[2] #Height 
  D = 1 #Third dimension of the data, e.g. 1 if black-white image
  x = normalize_images(x, H, W)

  #Reshape
  x = x.reshape(-1, H, W, 1).astype('float32')
  y = keras_utils.to_categorical(y) # encode one-hot vector

  #Zero-padding
  if padding:
    x = np.pad(x, ((0,0),(2,2),(2,2),(0,0)), 'constant')
    
  y = y.astype('float32')


  #Divide into train, validation and test dataset
  x_train = x[:train_size]
  y_train = y[:train_size]
  x_test = x[train_size:train_size+test_size]
  y_test = y[train_size:train_size+test_size]

  num_classes = y_train.shape[1]

  W = x.shape[1] #Width of image in pixels
  H = x.shape[2] #Height 

  return (x_train, y_train), (x_test, y_test), num_classes, H, W, D

"""**Loads a few images from the oral dataset.**"""

def load_real_data():
  imgs = []
  num_classes = 2
  path = "/content/drive/My Drive/OralCancer_DataSet3"
  path1 = "/train/Healthy"
  path2 = "/train/Cancer"
  path3 = "/test/Healthy"
  path4 = "/test/Cancer"
  paths = [path+path1, path+path2, path+path3, path+path4]
  valid_images = [".jpg", ".jpeg"]
  for i in range(4):
    path = paths[i]
    for f in os.listdir(path):
      ext = os.path.splitext(f)[1]
      if ext.lower() not in valid_images:
        continue
      imgs.append((Image.open(os.path.join(path,f)))) #Append and convert to numpy array
  imgs = np.stack(imgs, axis=0)
  x_train = imgs[0:20,...]/255.0 #Normalize image and split into train and test
  x_test = imgs[20:,...]/255.0
  y_train = np.concatenate([np.zeros(10), np.ones(10)], axis=0) #0 is healthy, 1 is malicious
  y_train = keras_utils.to_categorical(y_train, num_classes=2)
  y_test = y_train

  W = x_train.shape[1] #Width of image in pixels
  H = x_train.shape[2] #Height 
  D = x_train.shape[3] #Height 
  return x_train, x_test, y_train, y_test, num_classes, H, W, D

print(np.max(x_train[12]))
print(x_train.shape[1:])
plt.imshow(imgs[0,...])

"""**Plot first image from data**"""

def plot_image(data):
  first_image = data[0][0][0]
  first_image = np.array(first_image, dtype='float')
  pixels = first_image.reshape((28, 28))
  plt.imshow(pixels, cmap='gray')
  plt.show()

"""**Definition of ResNet-50**"""

def resnet_model(input_shape, num_classes):
  resnet = Sequential()
  resnet.add(keras.applications.resnet.ResNet50(include_top=False, weights= None, input_tensor=None, input_shape = input_shape, pooling='max', classes=num_classes))
  resnet.add(Dense(num_classes, activation = 'softmax'))
  return resnet

"""**Definition of VGG16**"""

def vgg16_model(input_shape ,num_classes):
  vgg16 = Sequential()
  vgg16.add(keras.applications.vgg16.VGG16(include_top=False, weights= None, input_tensor=None, input_shape = input_shape, pooling='max', classes=num_classes))
  vgg16.add(Dense(num_classes, activation = 'softmax'))
  return vgg16

"""**Definition of LeNet-5**"""

def lenet_model(input_shape, num_classes):
  lenet = keras.Sequential()
  lenet.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))
  lenet.add(MaxPool2D())
  lenet.add(Conv2D(filters=48, kernel_size=(5, 5), activation='relu'))
  lenet.add(MaxPool2D())
  lenet.add(Flatten())
  lenet.add(Dense(units=120, activation='relu'))
  lenet.add(Dense(units=84, activation='relu'))
  lenet.add(Dense(units=num_classes, activation = 'softmax'))
  return lenet

"""**Defines a multi-layer perceptron (MLP). Hidden units is a vector containing the number of hidden units between each layer.**"""

def MLP(hidden_units, input_shape, num_classes, dropout_rate = 0):
  MLP = Sequential()
  MLP.add(Flatten(input_shape = input_shape))
  MLP.add(Dense(hidden_units[0], activation = 'relu', input_shape = input_shape))
  MLP.add(BatchNormalization())
  MLP.add(Dropout(rate = dropout_rate))
  for i in range(len(hidden_units)-1):
    MLP.add(Dense(hidden_units[i+1], activation = 'relu'))
    MLP.add(BatchNormalization())
    MLP.add(Dropout(rate = dropout_rate))
  MLP.add(Dense(num_classes, activation = 'softmax'))
  return MLP

"""**Definition that creates and compiles the selected network. Specify train size etc.**"""

def run_model(network, dataset, method):  
  if dataset not in ['mnist', 'oral']:
    raise Exception('Dataset not available')
  if method in ['baseline', 'temp_scale']:
    if network not in ['lenet', 'vgg16', 'resnet']:
      raise Exception('Unimplemented network')
    y_pred, y_test = globals()[network](dataset)
    if method is 'temp_scale':
      y_pred, temp = run_temp_scale(y_pred, y_test)
  elif method is 'ensemble':
    if network not in ['lenet', 'vgg16', 'resnet', 'MLP']:
      raise Exception('Unimplemented network')
    y_pred, y_test = ensemble(network, dataset)
  else:
    raise Exception('Unimplemented method')
  return y_pred, y_test

"""**Builds baseline ResNet-50. Returns predictions and labels**"""

def resnet(dataset):
  train_size, test_size, epochs, batch_size = get_hyperparameters('resnet')
  x_train, x_test, y_train, y_test, num_classes, H, W, D = define_dataset(dataset, train_size, test_size)
  model = resnet_model((H, W, D), num_classes)
  y_pred, accuracy = compile_fit_predict(model, x_train, y_train, x_test, y_test, epochs, batch_size)
  return y_pred, y_test

"""**Builds baseline VGG-16. Returns predictions and labels**"""

def vgg16(dataset):
  train_size, test_size, epochs, batch_size = get_hyperparameters('vgg16')
  x_train, x_test, y_train, y_test, num_classes, H, W, D = define_dataset(dataset, train_size, test_size)
  model = vgg16_model((H, W, D), num_classes)
  y_pred, accuracy = compile_fit_predict(model, x_train, y_train, x_test, y_test, epochs, batch_size)
  return y_pred, y_test

"""**Builds baseline LeNet-5. Returns predictions and labels**"""

def lenet(dataset):
  train_size, test_size, epochs, batch_size = get_hyperparameters('lenet')
  x_train, x_test, y_train, y_test, num_classes, H, W, D = define_dataset(dataset, train_size, test_size)
  model = lenet_model((H, W, D), num_classes)
  y_pred, accuracy = compile_fit_predict(model, x_train, y_train, x_test, y_test, epochs, batch_size)
  return y_pred, y_test

"""**Builds ensemble using MLP. Returns predictions and labels.**"""

def ensemble(network, dataset):
  ensemble_members = 10
  count = 0
  it = 0
  train_size, test_size, epochs, batch_size = get_hyperparameters(network)
  x_train, x_test, y_train, y_test, num_classes, H, W, D = define_dataset(dataset, train_size, test_size)
  optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay = 1e-4) #kolla upp detta!
  prob_temp = np.zeros((test_size, num_classes))
  while count < ensemble_members:
    if network is 'MLP':
      input_shape = x_train.shape[1:]
      hidden_units = [200, 200, 200]
      model = MLP(hidden_units, input_shape, num_classes, dropout_rate = 0)
    elif network in ['lenet', 'vgg16', 'resnet']:
      model = globals()[network + "_model"]((H, W, D), num_classes)
    y_pred, accuracy = compile_fit_predict(model, x_train, y_train, x_test, y_test, epochs, batch_size, verbose = 0)
    if accuracy < 0.2:
      print("Ensemble member removed. Current number of ensemble members: ", count, '\n')
      continue 
    prob_temp += y_pred
    count += 1
    print("Ensemble member kept. Current number of ensemble members: ", count, '\n')
  y_pred = prob_temp/float(ensemble_members)
  return y_pred, y_test

"""**Return hyperparameters for a given network.**"""

def get_hyperparameters(network):
  if network is 'lenet':
    train_size = 900
    test_size = 40000
    epochs = 10
    batch_size = 64
  elif network is 'vgg16':
    train_size = 5000
    test_size = 40000
    epochs = 10
    batch_size = 64
  elif network is 'resnet':
    train_size = 6000
    test_size = 40000
    epochs = 10
    batch_size = 64
  elif network is 'MLP':
    train_size = 100 #****FIXA sizes*****
    test_size = 100
    batch_size = 64
    epochs = 10
  else:
    raise Exception('Unimplemented network')
  return train_size, test_size, epochs, batch_size

"""**Compiles, trains, evaluates and predict for a given model. Returns predictions and labels.**"""

def compile_fit_predict(model, x_train, y_train, x_test, y_test, epochs, batch_size, verbose = 1):
  test_batch_size = 32
  optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay = 1e-4) #kolla upp detta!
  model.compile(optimizer = optimizer,loss = 'categorical_crossentropy', metrics = ['accuracy'])
  #model.summary()
  start = time.time()
  model.fit(x = x_train, y = y_train, epochs = epochs, batch_size = batch_size, verbose = verbose)
  end = time.time()
  print("Training time: ", end - start, "seconds")
  start = time.time()
  loss_metrics = model.evaluate(x_test,y_test, batch_size = test_batch_size)
  end = time.time()
  print("Evaluation time: ", end - start, "seconds")
  print("Loss metrics:", loss_metrics)
  y_pred = model.predict(x_test)
  return y_pred, loss_metrics[1]

"""**Loads the chosen dataset**"""

def define_dataset(dataset, train_size, test_size):
  if(dataset is 'mnist'):
    (x_train, y_train), (x_test, y_test), num_classes, H, W, D = load_mnist_no_val(train_size, test_size, True)
  elif(dataset is 'oral'):
    raise Exception('Cannot choose sizes yet')
    #x_train, x_test, y_train, y_test, num_classes, H, W, D = load_real_data()
  else: 
    raise Exception('Dataset not found')
  return x_train, x_test, y_train, y_test, num_classes, H, W, D

"""**Returns the negative log-likelihood as implemented by scikit.**"""

def nll_score(y_pred, y_test):
  return sk_met.log_loss(y_test, y_pred, normalize = True)

"""**Brier score från** https://stats.stackexchange.com/questions/403544/how-to-compute-the-brier-score-for-more-than-two-classes"""

def brier_score(y_pred, y_test):
  return np.mean(np.sum((y_pred - y_test)**2, axis=1)) #**2 utför operationen elementvis

"""**Derive correctness, vector of length(number of samples). Element == true if correct classification**"""

def correctness_calc(y_pred, y_test):
  prediction = np.argmax(y_pred, axis=1)
  label = np.argmax(y_test, axis=-1)
  correctness = (label == prediction)
  return correctness

"""**Packs results from inference for calculations of AECE, demo: https://github.com/yding5/AdaptiveBinning/blob/master/demo.py. Also returns accuracy.**"""

def collect_confidence(y_pred, y_test):
  probability = y_pred
  prediction = np.argmax(probability, axis=1)
  label = np.argmax(y_test, axis=-1)
  infer_results = []

  for i in range(len(y_test)):
    correctness = (label[i] == prediction[i])
    infer_results.append([probability[i][prediction[i]], correctness])  
  
  return infer_results

"""**Computes certainty measures**"""

def certainty_measures(y_pred, y_test):
  NLL = nll_score(y_pred, y_test)
  brier = brier_score(y_pred, y_test)
  print("--------------------------------")
  print("NLL: ", NLL )
  print("Brier: ", brier)
  ECE, ece_confidence, ece_accuracy = ece_score(y_pred, y_test)
  print("ECE: ", ECE)
  AECE, AMCE, cof_min, cof_max, adaptive_confidence, adaptive_accuracy = AdaptiveBinning(collect_confidence(y_pred, y_test), False)
  print("AECE: ", AECE)
  print("AMCE: ", AMCE)
  print("--------------------------------")

  return NLL, brier, ECE, AECE, AMCE, ece_confidence, ece_accuracy, adaptive_confidence, adaptive_accuracy

"""**Returns the logits** https://github.com/google-research/google-research/blob/master/uq_benchmark_2019/uq_utils.py"""

def np_inverse_softmax(x):
  """Invert a softmax operation over the last axis of a np.ndarray."""
  return np.log(x / x[..., :1])

"""**Temperature scaling using session** https://github.com/ondrejba/tf_calibrate/blob/master/calibration.py"""

def temperature_scale(logits, session, valid_labels, learning_rate=0.01, num_steps=50):
    """
    Calibrate the confidence prediction using temperature scaling.
    :param logits:          Outputs of the neural network before softmax.
    :param session:         Tensorflow session.
    :param x_pl:            Placeholder for the inputs to the NN.
    :param y_pl:            Placeholder for the label for the loss.
    :param valid_data:      Validation inputs.
    :param valid_labels:    Validation labels.
    :return:                Scaled predictions op.
    """

    temperature = tf.Variable(initial_value=1., trainable=True, dtype=tf.float32, name="temperature")
    scaled_logits = logits / temperature

    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=valid_labels, logits=scaled_logits))

    opt = tf.train.MomentumOptimizer(learning_rate, 0.9)
    opt_step = opt.minimize(loss, var_list=[temperature])

    session.run(tf.variables_initializer([temperature]))
    session.run(tf.variables_initializer([opt.get_slot(var, name) for name in opt.get_slot_names() for var in [temperature]]))

    for i in range(num_steps):
        session.run(opt_step)

    return temperature

"""**Applies temperature scaling. Returns the scaled predictions and temperature.**"""

def run_temp_scale(y_pred,y_test):
  logits_test = np_inverse_softmax(y_pred)

  #Initialize sesstion and run
  init_op = tf.global_variables_initializer() 
  with tf.Session() as sess:
      sess.run(init_op)
      sparse_labels = np.argmax(y_test, axis=1) 
      temp = temperature_scale(logits_test, sess, sparse_labels, learning_rate=0.01, num_steps=50)
      print("Temperature: ", sess.run(temp))
      scaled_logits = logits_test / temp

      #Evaluates the values of temp and scaled_predictions
      temp = sess.run(temp)
      scaled_predictions = sess.run(tf.nn.softmax(scaled_logits))
      
  return scaled_predictions, temp

"""**Save data to file. Assumes one folder per network. One file in each network for every run of the method.**"""

def save_data(method, network, y_pred, y_test):
  NLL, brier, ECE, AECE, AMCE, ece_confidence, ece_accuracy, adaptive_confidence, adaptive_accuracy = certainty_measures(y_pred, y_test)
  a = {'method': method, 'network': network, 'y_pred': y_pred, 'y_test' : y_test, 'NLL': NLL, 'brier': brier, 
       'ECE': ECE, 'AECE': AECE, 'AMCE': AMCE, 'ece_confidence': ece_confidence, 'ece_accuracy': ece_accuracy, 
       'adaptive_confidence': adaptive_confidence, 'adaptive_accuracy': adaptive_accuracy}
  if not os.path.exists(network):  #Create directory if needed
    os.mkdir(network)
  np.savez_compressed(network + "/" + method +  "_result.npz", **a)

"""**Loads results from files for a network. Saves in dictionary, method is key.**"""

def load_results(network):
  result = {}
  methods = ['baseline', 'temp_scale', 'SWAG', 'ensemble']
  for method in methods:
      path = network + "/" + method + '_result.npz'
      if(os.path.exists(path)):
        x = np.load(path, allow_pickle=True)  
        result[method] = x      
  return result

"""**Reads predictions and correct labels. Returns results in same format as other methods.**"""

def load_swag_results(network):
  path = "SWAG_" + network + "_result_plot.npz"
  if not os.path.exists(path):
    raise Exception("Path does not exist!")
  x = np.load(path, allow_pickle=True)  
  y_pred = x['y_pred']
  y_test = x['y_test']

  save_data('SWAG', network, y_pred, y_test)

load_swag_results('lenet')

plot_reliability('lenet')

"""**Returns final ECE and acccuracies and confidence of each bin. Uses equidistant bins. https://github.com/gpleiss/temperature_scaling/blob/master/temperature_scaling.py**"""

def ece_score(y_pred, y_test, n_bins = 20):
  bin_boundaries = np.linspace(0, 1, n_bins + 1)
  bin_lowers = bin_boundaries[:-1]
  bin_uppers = bin_boundaries[1:]
  accuracy_array = []
  confidence_array = []

  confidences = np.amax(y_pred, axis =-1) #Value
  predictions = np.argmax(y_pred, axis = -1) #Index
  correctness = correctness_calc(y_pred, y_test) #True or false
  num_samples = y_pred.shape[0]
  ece = np.zeros(num_samples)
  for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
    in_bin = np.greater(confidences, bin_lower.item()) * np.less(confidences, bin_upper.item())
    sum_in_bin = np.sum(in_bin)
    if sum_in_bin > 0:
      prop_in_bin = np.mean(in_bin)
      accuracy_in_bin = np.mean(correctness[in_bin]) #Accuracy in bin
      avg_confidence_in_bin = np.mean(confidences[in_bin]) #Average confidence in bin

      #Append results
      accuracy_array.append(accuracy_in_bin) 
      confidence_array.append(avg_confidence_in_bin)
      ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
  ece = np.sum(ece)/num_samples
  return ece, confidence_array, accuracy_array

"""**Plots reliability diagram. (Confidence on x-axis, accuracy on y-axis)**"""

def plot_reliability(network):
    result = load_results(network) #Load results in dictionary

    if not (os.path.exists('Plots')): #Create directory if needed
      os.mkdir('Plots')

    factor = 1.5
    #Plot for ECE
    plt.figure(figsize = (8*factor,6*factor) )
    ax = plt.axes()
    line = np.linspace(0,1,50)
    ax.plot(line, line, linestyle = "--", color = "k", label = "calibrated".capitalize())
    plt.title("Reliability diagram, " + network.capitalize() + ', uniform bins', fontsize = 22)
    plt.xlabel("Confidence", fontsize = 20)
    plt.ylabel("Accuracy", fontsize = 20)
    plt.xlim(0,1)
    plt.ylim(0,1)
    ax.xaxis.set_minor_formatter(mpl.ticker.FormatStrFormatter(""))
    ax.tick_params(axis='both', which='major', labelsize=14)
    ax.tick_params(axis='both', which='minor', labelsize=14)

    
    for method in result.keys():
      conf = result[method]['ece_confidence']
      acc = result[method]['ece_accuracy']
      ax.plot(conf, acc, "-o", label = np.array2string(result[method]['method']).strip("\'").capitalize())
    plt.legend(fontsize = 18)
    plt.savefig('Plots/' + network + '_ECE.png') #Save figure

    #Plot for AECE
    plt.figure(figsize = (8*factor,6*factor))
    ax = plt.axes()
    ax.plot(line, line, linestyle = "--", color = "k", label = "calibrated".capitalize())
    plt.title("Reliability diagram, " + network.capitalize()+ ', adaptive bins', fontsize = 22)
    plt.xlabel("Confidence", fontsize = 20)
    plt.ylabel("Accuracy", fontsize = 20)
    plt.xlim(0,1)
    plt.ylim(0,1)
    ax.xaxis.set_minor_formatter(mpl.ticker.FormatStrFormatter(""))
    ax.tick_params(axis='both', which='major', labelsize=14)
    ax.tick_params(axis='both', which='minor', labelsize=14)

    for method in result.keys():
      conf = result[method]['adaptive_confidence']
      acc = result[method]['adaptive_accuracy']
      ax.plot(conf, acc, "-o", label = np.array2string(result[method]['method']).strip("\'").capitalize())
    plt.legend(fontsize = 18)
    plt.savefig('Plots/' + network + '_AECE.png') #Save figure

"""**------------------------------------------------------------------------------------------------------------------------------------------------------------------**

**Main, trains network and returns predictions**
"""

def main(network, dataset, method):
  y_pred, y_test = run_model(network, dataset, method)
  save_data(method, network, y_pred, y_test)
  return y_pred, y_test
network = 'lenet'
dataset = 'mnist'
method = 'ensemble'
print('In progress: ', method)
y_pred, y_test = main(network, dataset, method)
#plot_reliability(network)

plot_reliability('resnet')

#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'
!nvidia-smi